{"cells":[{"cell_type":"code","source":["# @title Cell 0: Mount Google Drive (for Colab use)\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"WMYmW1XygZw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Cell 1: Install Dependencies (for Colab use)\n","!pip install -r /content/drive/MyDrive/Product_Detector/requirements.txt"],"metadata":{"id":"r0elfbblgasX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Cell 2: Imports\n","\n","# File and path handling\n","import os\n","\n","# Image and video processing\n","import cv2\n","import numpy as np\n","\n","# Detection framework (YOLOv8)\n","import torch\n","from ultralytics import YOLO\n","\n","# Dataset loading (COCO) for class verification\n","from datasets import load_dataset\n","\n","# Quick web interface\n","import gradio as gr\n","\n","# Typing and utilities\n","from typing import List, Tuple, Dict"],"metadata":{"id":"NWSppt3igfjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Step 0: Load and explore the COCO dataset (rafaelpadilla/coco2017)\n","\n","# Load the YOLOv8 model to access the full list of COCO class names\n","model = YOLO(\"yolov8n.pt\")\n","\n","# List of class names you want to exclude from detection\n","EXCLUDED_CLASSES = [\"apple\", \"orange\", \"hot dog\", \"skateboard\", \"laptop\" ]  # <-- modificá acá según tus necesidades\n","\n","# Generate the list of target classes excluding the ones you don't want\n","TARGET_CLASSES = [name for name in model.names.values() if name not in EXCLUDED_CLASSES]\n","\n","print(f\"Using {len(TARGET_CLASSES)} COCO classes (excluding {len(EXCLUDED_CLASSES)}):\")\n","print(TARGET_CLASSES)\n","\n"],"metadata":{"id":"QUpF9GDhghRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Step 1: Load and configure the YOLOv8 model\n","\n","model = YOLO(\"yolov8n.pt\")  # Nano version pretrained on COCO\n","model.conf = 0.38           # Confidence threshold: discard detections with score < 0.5\n","\n","print(f\"Model loaded with confidence threshold {model.conf}\")"],"metadata":{"id":"OdVnKNFOglki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Step 2: IoU-based tracker for product detection\n","from typing import List\n","import numpy as np\n","\n","def compute_iou(box1: np.ndarray, box2: np.ndarray) -> float:\n","    \"\"\"\n","    Compute the Intersection over Union (IoU) between two boxes.\n","    Boxes are in [x1, y1, x2, y2] format.\n","    \"\"\"\n","    x1 = max(box1[0], box2[0])\n","    y1 = max(box1[1], box2[1])\n","    x2 = min(box1[2], box2[2])\n","    y2 = min(box1[3], box2[3])\n","\n","    inter_w = max(0, x2 - x1)\n","    inter_h = max(0, y2 - y1)\n","    inter_area = inter_w * inter_h\n","\n","    area1 = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n","    area2 = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n","\n","    union_area = area1 + area2 - inter_area\n","    return inter_area / union_area if union_area > 0 else 0.0\n","\n","class SimpleIoUTracker:\n","    \"\"\"\n","    Simple IoU-based tracker to assign unique IDs to boxes\n","    across consecutive frames.\n","    \"\"\"\n","    def __init__(self, iou_threshold: float = 0.5):\n","        self.iou_threshold = iou_threshold\n","        self.next_id = 0\n","        self.prev_boxes: List[np.ndarray] = []\n","        self.prev_ids: List[int] = []\n","\n","    def update(self, boxes: List[np.ndarray]) -> List[int]:\n","        \"\"\"\n","        Assign IDs to new boxes based on IoU with previous boxes.\n","        boxes: list of [x1, y1, x2, y2] arrays for the current frame.\n","        Returns: list of assigned IDs in the same order.\n","        \"\"\"\n","        assigned_ids: List[int] = []\n","        unmatched_prev = list(range(len(self.prev_boxes)))\n","\n","        for box in boxes:\n","            best_iou = 0.0\n","            best_idx = -1\n","            for idx in unmatched_prev:\n","                iou = compute_iou(box, self.prev_boxes[idx])\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_idx = idx\n","\n","            if best_iou >= self.iou_threshold and best_idx != -1:\n","                assigned_id = self.prev_ids[best_idx]\n","                unmatched_prev.remove(best_idx)\n","            else:\n","                assigned_id = self.next_id\n","                self.next_id += 1\n","\n","            assigned_ids.append(assigned_id)\n","\n","        # Update state for next frame\n","        self.prev_boxes = boxes.copy()\n","        self.prev_ids = assigned_ids.copy()\n","        return assigned_ids\n","\n","# Test the tracker\n","tracker = SimpleIoUTracker(iou_threshold=0.3)\n","boxes_frame1 = [np.array([10,10,50,50]), np.array([100,100,150,150])]\n","ids1 = tracker.update(boxes_frame1)\n","boxes_frame2 = [np.array([12,12,52,52]), np.array([200,200,250,250])]\n","ids2 = tracker.update(boxes_frame2)\n","print(ids1, ids2)  # expect [0, 1] then [0, 2]"],"metadata":{"id":"zXmD2woDgn8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Step 3: Initialize the tracker and global accumulator, corrected to use TARGET_CLASSES\n","\n","tracker = SimpleIoUTracker(iou_threshold=0.5)\n","detected_products = set()\n","\n","def process_frame(frame: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Process a frame: detect products, assign IDs, accumulate list, and render annotations.\n","    Now filters on TARGET_CLASSES (all COCO classes).\n","    \"\"\"\n","    # YOLOv8 inference\n","    results = model(frame)\n","    det     = results[0]\n","    boxes   = det.boxes.xyxy.cpu().numpy()\n","    class_ids = det.boxes.cls.cpu().numpy().astype(int)\n","    scores    = det.boxes.conf.cpu().numpy()\n","\n","    # Filter detections only for our TARGET_CLASSES and above confidence threshold\n","    filtered_boxes = []\n","    filtered_names = []\n","    for box, cls_id, score in zip(boxes, class_ids, scores):\n","        name = model.names[cls_id]\n","        if name in TARGET_CLASSES and score >= model.conf:\n","            filtered_boxes.append(box.astype(int))\n","            filtered_names.append(name)\n","\n","    # IoU tracking\n","    ids = tracker.update(filtered_boxes)\n","\n","    # Accumulate unique detected products\n","    for name in filtered_names:\n","        detected_products.add(name)\n","\n","    # Draw bounding boxes and labels\n","    for box, name, id_ in zip(filtered_boxes, filtered_names, ids):\n","        x1, y1, x2, y2 = box\n","        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","        cv2.putText(frame, f\"{name}-{id_}\", (x1, y1 - 10),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\n","    # Display the list of detected classes in the top-left corner\n","    y0 = 30\n","    for prod in sorted(detected_products):\n","        cv2.putText(frame, prod, (10, y0),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n","        y0 += 25\n","\n","    return frame\n"],"metadata":{"id":"eiZ5cE1Wgot2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Step 4: Video processing function\n","import cv2\n","import tempfile\n","\n","def process_video(video_path: str) -> str:\n","    \"\"\"\n","    Takes a video path, processes frame by frame with process_frame(),\n","    writes a new annotated video file, and returns its path.\n","    \"\"\"\n","    # Open input video\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise RuntimeError(f\"Could not open video: {video_path}\")\n","\n","    # Video properties\n","    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps    = cap.get(cv2.CAP_PROP_FPS) or 24.0\n","\n","    # Prepare writer with a temporary file\n","    tmp_file = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False)\n","    out_path = tmp_file.name\n","    fourcc   = cv2.VideoWriter_fourcc(*\"mp4v\")\n","    writer   = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n","\n","    # Processing loop\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        annotated = process_frame(frame)\n","        writer.write(annotated)\n","\n","    # Release resources\n","    cap.release()\n","    writer.release()\n","\n","    return out_path"],"metadata":{"id":"WP6K8piXgr7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Step 5: Web interface with Gradio\n","import gradio as gr\n","from gradio.components import Video\n","\n","interface = gr.Interface(\n","    fn=process_video,\n","    inputs=Video(label=\"Upload your shelf video\"),\n","    outputs=Video(label=\"Annotated Video\"),\n","    title=\"Product Detector in Video\",\n","    description=\"Upload a supermarket shelf clip and the AI will list the detected products.\",\n","    allow_flagging=\"never\"\n",")\n","\n","interface.launch(debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYPPR7ZTgvJG","outputId":"d828d4e2-564a-4eda-ccad-3f9ed0127dce","executionInfo":{"status":"ok","timestamp":1747356437012,"user_tz":180,"elapsed":138,"user":{"displayName":"Gonzalo Hadad","userId":"04139227734845046718"}}},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://fd5f21a839eff8729f.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":40}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1oTC7cSMik7ngW7XxOmmwcFioGvuBjS_i","timestamp":1747316278561}],"authorship_tag":"ABX9TyOqB/Am7IjvSMVqy3oV2370"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}